# MLP regressor configuration - LARGE model with heavier regularization
# Larger architecture with more dropout for Phase 1 improvements
# Input: 1792 (512 + 1280) -> 1024 -> 512 -> 256 -> 1

# Hidden layer dimensions
hidden_dims:
  - 1024
  - 512
  - 256

# Dropout probability before final output layer (increased for regularization)
dropout: 0.3
